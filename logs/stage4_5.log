----------------------- Stage 4 begin: lang model ---------------------------
Mon May  9 17:51:56 UTC 2022
local/tamil_train_lm.sh 
Not installing the pocolm toolkit since it is already there.
local/tamil_train_lm.sh: Getting the Data sources
local/tamil_train_lm.sh: training the unpruned LM
lm_name wordlist_4_train-2_ted-1
unpruned_lm_dir: data/local/local_lm/data/wordlist_4_train-2_ted-1.pocolm
train_lm.py starting 
Mon May  9 17:51:59 UTC 2022
get_counts.py: extending min-counts from 2.0,2.0 to 2.0,2.0 since ngram order is 4
get_counts.py: extending min-counts from 1.0,1.0 to 1.0,1.0 since ngram order is 4
Mon May  9 18:41:49 UTC 2022
train_lm.py ended 
get_data_prob.py starting 
Mon May  9 18:41:49 UTC 2022
get_data_prob.py: log-prob of data/local/local_lm/data/real_dev_set.txt given model data/local/local_lm/data/wordlist_4_train-2_ted-1.pocolm was -7.647581645297975 per word [perplexity = 2095.5716210822075] over 109506.0 words.
Mon May  9 18:42:01 UTC 2022
get_data_prob.py ended 
local/tamil_train_lm.sh: pruning the LM (to larger size)
