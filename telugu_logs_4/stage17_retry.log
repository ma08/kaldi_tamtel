nohup: ignoring input
----------------------- Stage 17 begin---------------------------
Fri Jul 15 10:48:02 EDT 2022
local/chain/run_tdnn.sh: data_dir: telugu_data  exp_dir : telugu_exp
local/chain/run_tdnn.sh 
local/chain/run_tdnn.sh: running on telugu_data.
local/nnet3/run_ivector_common_telugu.sh: data_dir: telugu_data  exp_dir : telugu_exp nnet3_affix : _cleaned_1d
local/nnet3/run_ivector_common_telugu.sh: preparing directory for low-resolution speed-perturbed data (for alignment)
fix_data_dir.sh: kept all 44335 utterances.
fix_data_dir.sh: old files are kept in telugu_data/train_cleaned/.backup
utils/data/perturb_data_dir_speed_3way.sh: making sure the utt2dur and the reco2dur files are present
... in telugu_data/train_cleaned, because obtaining it after speed-perturbing
... would be very slow, and you might need them.
utils/data/get_utt2dur.sh: telugu_data/train_cleaned/utt2dur already exists with the expected length.  We won't recompute it.
utils/data/get_reco2dur.sh: telugu_data/train_cleaned/reco2dur already exists with the expected length.  We won't recompute it.
utils/data/perturb_data_dir_speed.sh: generated speed-perturbed version of data in telugu_data/train_cleaned, in telugu_data/train_cleaned_sp_speed0.9
fix_data_dir.sh: kept all 44335 utterances.
fix_data_dir.sh: old files are kept in telugu_data/train_cleaned_sp_speed0.9/.backup
utils/validate_data_dir.sh: Successfully validated data-directory telugu_data/train_cleaned_sp_speed0.9
utils/data/perturb_data_dir_speed.sh: generated speed-perturbed version of data in telugu_data/train_cleaned, in telugu_data/train_cleaned_sp_speed1.1
fix_data_dir.sh: kept all 44335 utterances.
fix_data_dir.sh: old files are kept in telugu_data/train_cleaned_sp_speed1.1/.backup
utils/validate_data_dir.sh: Successfully validated data-directory telugu_data/train_cleaned_sp_speed1.1
utils/data/combine_data.sh telugu_data/train_cleaned_sp telugu_data/train_cleaned telugu_data/train_cleaned_sp_speed0.9 telugu_data/train_cleaned_sp_speed1.1
utils/data/combine_data.sh: combined utt2uniq
utils/data/combine_data.sh: combined segments
utils/data/combine_data.sh: combined utt2spk
utils/data/combine_data.sh [info]: not combining utt2lang as it does not exist
utils/data/combine_data.sh: combined utt2dur
utils/data/combine_data.sh [info]: **not combining utt2num_frames as it does not exist everywhere**
utils/data/combine_data.sh: combined reco2dur
utils/data/combine_data.sh [info]: **not combining feats.scp as it does not exist everywhere**
utils/data/combine_data.sh: combined text
utils/data/combine_data.sh [info]: **not combining cmvn.scp as it does not exist everywhere**
utils/data/combine_data.sh [info]: not combining vad.scp as it does not exist
utils/data/combine_data.sh: combined reco2file_and_channel
utils/data/combine_data.sh: combined wav.scp
utils/data/combine_data.sh [info]: not combining spk2gender as it does not exist
fix_data_dir.sh: kept all 133005 utterances.
fix_data_dir.sh: old files are kept in telugu_data/train_cleaned_sp/.backup
utils/data/perturb_data_dir_speed_3way.sh: generated 3-way speed-perturbed version of data in telugu_data/train_cleaned, in telugu_data/train_cleaned_sp
utils/validate_data_dir.sh: Successfully validated data-directory telugu_data/train_cleaned_sp
fix_data_dir.sh: kept all 133005 utterances.
fix_data_dir.sh: old files are kept in telugu_data/train_cleaned_sp/.backup
utils/copy_data_dir.sh: copied data from telugu_data/train_cleaned_sp to telugu_data/train_cleaned_sp_hires
utils/validate_data_dir.sh: Successfully validated data-directory telugu_data/train_cleaned_sp_hires
utils/fix_data_dir.sh: filtered /tmp/kaldi.TV4d/speakers from 4534 to 4533 lines based on filter telugu_data/dev/cmvn.scp.
utils/fix_data_dir.sh: filtered telugu_data/dev/spk2utt from 4534 to 4533 lines based on filter /tmp/kaldi.TV4d/speakers.
fix_data_dir.sh: kept all 4930 utterances.
utils/fix_data_dir.sh: filtered telugu_data/dev/wav.scp from 4931 to 4930 lines based on filter /tmp/kaldi.TV4d/recordings.
utils/fix_data_dir.sh: filtered telugu_data/dev/reco2file_and_channel from 4931 to 4930 lines based on filter /tmp/kaldi.TV4d/recordings.
fix_data_dir.sh: old files are kept in telugu_data/dev/.backup
utils/copy_data_dir.sh: copied data from telugu_data/dev to telugu_data/dev_hires
utils/validate_data_dir.sh: Successfully validated data-directory telugu_data/dev_hires
utils/fix_data_dir.sh: filtered /tmp/kaldi.wrkz/speakers from 3085 to 3084 lines based on filter telugu_data/test/cmvn.scp.
utils/fix_data_dir.sh: filtered telugu_data/test/spk2utt from 3085 to 3084 lines based on filter /tmp/kaldi.wrkz/speakers.
fix_data_dir.sh: kept all 3482 utterances.
utils/fix_data_dir.sh: filtered telugu_data/test/wav.scp from 3483 to 3482 lines based on filter /tmp/kaldi.wrkz/recordings.
utils/fix_data_dir.sh: filtered telugu_data/test/reco2file_and_channel from 3483 to 3482 lines based on filter /tmp/kaldi.wrkz/recordings.
fix_data_dir.sh: old files are kept in telugu_data/test/.backup
utils/copy_data_dir.sh: copied data from telugu_data/test to telugu_data/test_hires
utils/validate_data_dir.sh: Successfully validated data-directory telugu_data/test_hires
local/nnet3/run_ivector_common_telugu.sh: making MFCC features for low-resolution speed-perturbed data
steps/make_mfcc.sh --nj 15 --cmd run.pl telugu_data/train_cleaned_sp
utils/validate_data_dir.sh: Successfully validated data-directory telugu_data/train_cleaned_sp
steps/make_mfcc.sh [info]: segments file exists: using that.
steps/make_mfcc.sh: Succeeded creating MFCC features for train_cleaned_sp
steps/compute_cmvn_stats.sh telugu_data/train_cleaned_sp
Succeeded creating CMVN stats for train_cleaned_sp
local/nnet3/run_ivector_common_telugu.sh: fixing input data-dir to remove nonexistent features, in case some 
.. speed-perturbed segments were too short.
fix_data_dir.sh: kept all 133005 utterances.
fix_data_dir.sh: old files are kept in telugu_data/train_cleaned_sp/.backup
local/nnet3/run_ivector_common_telugu.sh: aligning with the perturbed low-resolution data
steps/align_fmllr.sh --nj 15 --cmd run.pl telugu_data/train_cleaned_sp telugu_data/lang telugu_exp/tri3_cleaned telugu_exp/tri3_cleaned_ali_train_cleaned_sp
steps/align_fmllr.sh: feature type is lda
steps/align_fmllr.sh: compiling training graphs
steps/align_fmllr.sh: aligning data in telugu_data/train_cleaned_sp using telugu_exp/tri3_cleaned/final.alimdl and speaker-independent features.
steps/align_fmllr.sh: computing fMLLR transforms
steps/align_fmllr.sh: doing final alignment.
steps/align_fmllr.sh: done aligning data.
steps/diagnostic/analyze_alignments.sh --cmd run.pl telugu_data/lang telugu_exp/tri3_cleaned_ali_train_cleaned_sp
analyze_phone_length_stats.py: WARNING: optional-silence SIL is seen only 69.0189911515% of the time at utterance end.  This may not be optimal.
steps/diagnostic/analyze_alignments.sh: see stats in telugu_exp/tri3_cleaned_ali_train_cleaned_sp/log/analyze_alignments.log
1 warnings in telugu_exp/tri3_cleaned_ali_train_cleaned_sp/log/analyze_alignments.log
1738 warnings in telugu_exp/tri3_cleaned_ali_train_cleaned_sp/log/align_pass2.*.log
1875 warnings in telugu_exp/tri3_cleaned_ali_train_cleaned_sp/log/align_pass1.*.log
102628 warnings in telugu_exp/tri3_cleaned_ali_train_cleaned_sp/log/fmllr.*.log
local/nnet3/run_ivector_common_telugu.sh: creating high-resolution MFCC features
utils/data/perturb_data_dir_volume.sh: added volume perturbation to the data in telugu_data/train_cleaned_sp_hires
steps/make_mfcc.sh --nj 15 --mfcc-config conf/mfcc_hires.conf --cmd run.pl telugu_data/train_cleaned_sp_hires
utils/validate_data_dir.sh: Successfully validated data-directory telugu_data/train_cleaned_sp_hires
steps/make_mfcc.sh [info]: segments file exists: using that.
steps/make_mfcc.sh: Succeeded creating MFCC features for train_cleaned_sp_hires
steps/compute_cmvn_stats.sh telugu_data/train_cleaned_sp_hires
Succeeded creating CMVN stats for train_cleaned_sp_hires
fix_data_dir.sh: kept all 133005 utterances.
fix_data_dir.sh: old files are kept in telugu_data/train_cleaned_sp_hires/.backup
steps/make_mfcc.sh --nj 15 --mfcc-config conf/mfcc_hires.conf --cmd run.pl telugu_data/dev_hires
steps/make_mfcc.sh: moving telugu_data/dev_hires/feats.scp to telugu_data/dev_hires/.backup
utils/validate_data_dir.sh: Successfully validated data-directory telugu_data/dev_hires
steps/make_mfcc.sh [info]: segments file exists: using that.
steps/make_mfcc.sh: Succeeded creating MFCC features for dev_hires
steps/compute_cmvn_stats.sh telugu_data/dev_hires
Succeeded creating CMVN stats for dev_hires
fix_data_dir.sh: kept all 4930 utterances.
fix_data_dir.sh: old files are kept in telugu_data/dev_hires/.backup
steps/make_mfcc.sh --nj 15 --mfcc-config conf/mfcc_hires.conf --cmd run.pl telugu_data/test_hires
steps/make_mfcc.sh: moving telugu_data/test_hires/feats.scp to telugu_data/test_hires/.backup
utils/validate_data_dir.sh: Successfully validated data-directory telugu_data/test_hires
steps/make_mfcc.sh [info]: segments file exists: using that.
steps/make_mfcc.sh: Succeeded creating MFCC features for test_hires
steps/compute_cmvn_stats.sh telugu_data/test_hires
Succeeded creating CMVN stats for test_hires
fix_data_dir.sh: kept all 3482 utterances.
fix_data_dir.sh: old files are kept in telugu_data/test_hires/.backup
local/nnet3/run_ivector_common_telugu.sh: computing a subset of data to train the diagonal UBM.
utils/data/subset_data_dir.sh: reducing #utt from 133005 to 33251
local/nnet3/run_ivector_common_telugu.sh: computing a PCA transform from the hires data.
steps/online/nnet2/get_pca_transform.sh --cmd run.pl --splice-opts --left-context=3 --right-context=3 --max-utts 10000 --subsample 2 telugu_exp/nnet3_cleaned_1d/diag_ubm/train_cleaned_sp_hires_subset telugu_exp/nnet3_cleaned_1d/pca_transform
Done estimating PCA transform in telugu_exp/nnet3_cleaned_1d/pca_transform
local/nnet3/run_ivector_common_telugu.sh: training the diagonal UBM.
steps/online/nnet2/train_diag_ubm.sh --cmd run.pl --nj 30 --num-frames 700000 --num-threads 8 telugu_exp/nnet3_cleaned_1d/diag_ubm/train_cleaned_sp_hires_subset 512 telugu_exp/nnet3_cleaned_1d/pca_transform telugu_exp/nnet3_cleaned_1d/diag_ubm
steps/online/nnet2/train_diag_ubm.sh: Directory telugu_exp/nnet3_cleaned_1d/diag_ubm already exists. Backing up diagonal UBM in telugu_exp/nnet3_cleaned_1d/diag_ubm/backup.zID
steps/online/nnet2/train_diag_ubm.sh: initializing model from E-M in memory, 
steps/online/nnet2/train_diag_ubm.sh: starting from 256 Gaussians, reaching 512;
steps/online/nnet2/train_diag_ubm.sh: for 20 iterations, using at most 700000 frames of data
Getting Gaussian-selection info
steps/online/nnet2/train_diag_ubm.sh: will train for 4 iterations, in parallel over
steps/online/nnet2/train_diag_ubm.sh: 30 machines, parallelized with 'run.pl'
steps/online/nnet2/train_diag_ubm.sh: Training pass 0
steps/online/nnet2/train_diag_ubm.sh: Training pass 1
steps/online/nnet2/train_diag_ubm.sh: Training pass 2
steps/online/nnet2/train_diag_ubm.sh: Training pass 3
local/nnet3/run_ivector_common_telugu.sh: training the iVector extractor
steps/online/nnet2/train_ivector_extractor.sh --cmd run.pl --nj 15 --num-threads 4 --num-processes 2 --online-cmvn-iextractor true telugu_data/train_cleaned_sp_hires telugu_exp/nnet3_cleaned_1d/diag_ubm telugu_exp/nnet3_cleaned_1d/extractor
steps/online/nnet2/train_ivector_extractor.sh: doing Gaussian selection and posterior computation
Accumulating stats (pass 0)
Summing accs (pass 0)
Updating model (pass 0)
Accumulating stats (pass 1)
Summing accs (pass 1)
Updating model (pass 1)
Accumulating stats (pass 2)
Summing accs (pass 2)
Updating model (pass 2)
Accumulating stats (pass 3)
Summing accs (pass 3)
Updating model (pass 3)
Accumulating stats (pass 4)
Summing accs (pass 4)
Updating model (pass 4)
Accumulating stats (pass 5)
Summing accs (pass 5)
Updating model (pass 5)
Accumulating stats (pass 6)
Summing accs (pass 6)
Updating model (pass 6)
Accumulating stats (pass 7)
Summing accs (pass 7)
Updating model (pass 7)
Accumulating stats (pass 8)
Summing accs (pass 8)
Updating model (pass 8)
Accumulating stats (pass 9)
Summing accs (pass 9)
Updating model (pass 9)
utils/data/modify_speaker_info.sh: copied data from telugu_data/train_cleaned_sp_hires to telugu_exp/nnet3_cleaned_1d/ivectors_train_cleaned_sp_hires/train_cleaned_sp_hires_max2, number of speakers changed from 119412 to 124764
utils/validate_data_dir.sh: Successfully validated data-directory telugu_exp/nnet3_cleaned_1d/ivectors_train_cleaned_sp_hires/train_cleaned_sp_hires_max2
steps/online/nnet2/extract_ivectors_online.sh --cmd run.pl --nj 15 telugu_exp/nnet3_cleaned_1d/ivectors_train_cleaned_sp_hires/train_cleaned_sp_hires_max2 telugu_exp/nnet3_cleaned_1d/extractor telugu_exp/nnet3_cleaned_1d/ivectors_train_cleaned_sp_hires
steps/online/nnet2/extract_ivectors_online.sh: extracting iVectors
steps/online/nnet2/extract_ivectors_online.sh: combining iVectors across jobs
steps/online/nnet2/extract_ivectors_online.sh: done extracting (online) iVectors to telugu_exp/nnet3_cleaned_1d/ivectors_train_cleaned_sp_hires using the extractor in telugu_exp/nnet3_cleaned_1d/extractor.
steps/online/nnet2/extract_ivectors_online.sh --cmd run.pl --nj 15 telugu_data/dev_hires telugu_exp/nnet3_cleaned_1d/extractor telugu_exp/nnet3_cleaned_1d/ivectors_dev_hires
steps/online/nnet2/extract_ivectors_online.sh: extracting iVectors
steps/online/nnet2/extract_ivectors_online.sh: combining iVectors across jobs
steps/online/nnet2/extract_ivectors_online.sh: done extracting (online) iVectors to telugu_exp/nnet3_cleaned_1d/ivectors_dev_hires using the extractor in telugu_exp/nnet3_cleaned_1d/extractor.
steps/online/nnet2/extract_ivectors_online.sh --cmd run.pl --nj 15 telugu_data/test_hires telugu_exp/nnet3_cleaned_1d/extractor telugu_exp/nnet3_cleaned_1d/ivectors_test_hires
steps/online/nnet2/extract_ivectors_online.sh: extracting iVectors
steps/online/nnet2/extract_ivectors_online.sh: combining iVectors across jobs
steps/online/nnet2/extract_ivectors_online.sh: done extracting (online) iVectors to telugu_exp/nnet3_cleaned_1d/ivectors_test_hires using the extractor in telugu_exp/nnet3_cleaned_1d/extractor.
local/chain/run_tdnn.sh: creating lang directory with one state per phone.
steps/align_fmllr_lats.sh --nj 100 --cmd run.pl telugu_data/train_cleaned_sp telugu_data/lang telugu_exp/tri3_cleaned telugu_exp/chain_cleaned_1d/tri3_cleaned_train_cleaned_sp_lats
steps/align_fmllr_lats.sh: feature type is lda
steps/align_fmllr_lats.sh: compiling training graphs
steps/align_fmllr_lats.sh: aligning data in telugu_data/train_cleaned_sp using telugu_exp/tri3_cleaned/final.alimdl and speaker-independent features.
steps/align_fmllr_lats.sh: computing fMLLR transforms
steps/align_fmllr_lats.sh: generating lattices containing alternate pronunciations.
steps/align_fmllr_lats.sh: done generating lattices from training transcripts.
2001 warnings in telugu_exp/chain_cleaned_1d/tri3_cleaned_train_cleaned_sp_lats/log/align_pass1.*.log
116 warnings in telugu_exp/chain_cleaned_1d/tri3_cleaned_train_cleaned_sp_lats/log/generate_lattices.*.log
102637 warnings in telugu_exp/chain_cleaned_1d/tri3_cleaned_train_cleaned_sp_lats/log/fmllr.*.log
steps/nnet3/chain/build_tree.sh --frame-subsampling-factor 3 --context-opts --context-width=2 --central-position=1 --cmd run.pl 4000 telugu_data/train_cleaned_sp telugu_data/lang_chain telugu_exp/tri3_cleaned_ali_train_cleaned_sp telugu_exp/chain_cleaned_1d/tree_bi
steps/nnet3/chain/build_tree.sh: feature type is lda
steps/nnet3/chain/build_tree.sh: Using transforms from telugu_exp/tri3_cleaned_ali_train_cleaned_sp
steps/nnet3/chain/build_tree.sh: Initializing monophone model (for alignment conversion, in case topology changed)
steps/nnet3/chain/build_tree.sh: Accumulating tree stats
steps/nnet3/chain/build_tree.sh: Getting questions for tree clustering.
steps/nnet3/chain/build_tree.sh: Building the tree
steps/nnet3/chain/build_tree.sh: Initializing the model
steps/nnet3/chain/build_tree.sh: Converting alignments from telugu_exp/tri3_cleaned_ali_train_cleaned_sp to use current tree
steps/nnet3/chain/build_tree.sh: Done building tree
local/chain/run_tdnn.sh: creating neural net configs using the xconfig parser
tree-info telugu_exp/chain_cleaned_1d/tree_bi/tree 
steps/nnet3/xconfig_to_configs.py --xconfig-file telugu_exp/chain_cleaned_1d/tdnn1d_sp/configs/network.xconfig --config-dir telugu_exp/chain_cleaned_1d/tdnn1d_sp/configs/
nnet3-init telugu_exp/chain_cleaned_1d/tdnn1d_sp/configs//init.config telugu_exp/chain_cleaned_1d/tdnn1d_sp/configs//init.raw 
LOG (nnet3-init[5.5.1035~1-3dd90]:main():nnet3-init.cc:80) Initialized raw neural net and wrote it to telugu_exp/chain_cleaned_1d/tdnn1d_sp/configs//init.raw
nnet3-info telugu_exp/chain_cleaned_1d/tdnn1d_sp/configs//init.raw 
nnet3-init telugu_exp/chain_cleaned_1d/tdnn1d_sp/configs//ref.config telugu_exp/chain_cleaned_1d/tdnn1d_sp/configs//ref.raw 
LOG (nnet3-init[5.5.1035~1-3dd90]:main():nnet3-init.cc:80) Initialized raw neural net and wrote it to telugu_exp/chain_cleaned_1d/tdnn1d_sp/configs//ref.raw
nnet3-info telugu_exp/chain_cleaned_1d/tdnn1d_sp/configs//ref.raw 
nnet3-init telugu_exp/chain_cleaned_1d/tdnn1d_sp/configs//ref.config telugu_exp/chain_cleaned_1d/tdnn1d_sp/configs//ref.raw 
LOG (nnet3-init[5.5.1035~1-3dd90]:main():nnet3-init.cc:80) Initialized raw neural net and wrote it to telugu_exp/chain_cleaned_1d/tdnn1d_sp/configs//ref.raw
nnet3-info telugu_exp/chain_cleaned_1d/tdnn1d_sp/configs//ref.raw 
Fri Jul 15 15:46:56 EDT 2022
2022-07-15 15:46:56,503 [/rmespch_train/sourya/kaldi/egs/tamil_telugu_proj/s5_r3/steps/nnet3/chain/train.py:35 - <module> - INFO ] Starting chain model trainer (train.py)
2022-07-15 15:46:56,524 [/rmespch_train/sourya/kaldi/egs/tamil_telugu_proj/s5_r3/steps/nnet3/chain/train.py:284 - train - INFO ] Arguments for the experiment
{'alignment_subsampling_factor': 3,
 'apply_deriv_weights': False,
 'backstitch_training_interval': 1,
 'backstitch_training_scale': 0.0,
 'chain_opts': '',
 'chunk_left_context': 0,
 'chunk_left_context_initial': -1,
 'chunk_right_context': 0,
 'chunk_right_context_final': -1,
 'chunk_width': '150,110,100',
 'cleanup': True,
 'cmvn_opts': '--config=conf/online_cmvn.conf',
 'combine_sum_to_one_penalty': 0.0,
 'command': 'run.pl',
 'compute_per_dim_accuracy': False,
 'deriv_truncate_margin': None,
 'dir': 'telugu_exp/chain_cleaned_1d/tdnn1d_sp',
 'do_final_combination': True,
 'dropout_schedule': '0,0@0.20,0.5@0.50,0',
 'egs_command': None,
 'egs_dir': None,
 'egs_nj': 0,
 'egs_opts': '--frames-overlap-per-eg 0 --constrained false --online-cmvn true',
 'egs_stage': 0,
 'email': None,
 'exit_stage': None,
 'feat_dir': 'telugu_data/train_cleaned_sp_hires',
 'final_effective_lrate': 2.5e-05,
 'frame_subsampling_factor': 3,
 'frames_per_iter': 5000000,
 'initial_effective_lrate': 0.00025,
 'input_model': None,
 'l2_regularize': 0.0,
 'lat_dir': 'telugu_exp/chain_cleaned_1d/tri3_cleaned_train_cleaned_sp_lats',
 'leaky_hmm_coefficient': 0.1,
 'left_deriv_truncate': None,
 'left_tolerance': 5,
 'lm_opts': '--num-extra-lm-states=2000',
 'max_lda_jobs': 10,
 'max_models_combine': 20,
 'max_objective_evaluations': 30,
 'max_param_change': 2.0,
 'momentum': 0.0,
 'num_chunk_per_minibatch': '64',
 'num_epochs': 3.0,
 'num_jobs_final': 1,
 'num_jobs_initial': 1,
 'num_jobs_step': 1,
 'online_ivector_dir': 'telugu_exp/nnet3_cleaned_1d/ivectors_train_cleaned_sp_hires',
 'preserve_model_interval': 100,
 'presoftmax_prior_scale_power': -0.25,
 'proportional_shrink': 0.0,
 'rand_prune': 4.0,
 'remove_egs': True,
 'reporting_interval': 0.1,
 'right_tolerance': 5,
 'samples_per_iter': 400000,
 'shrink_saturation_threshold': 0.4,
 'shrink_value': 1.0,
 'shuffle_buffer_size': 5000,
 'srand': 0,
 'stage': -10,
 'train_opts': ['--optimization.memory-compression-level=2'],
 'tree_dir': 'telugu_exp/chain_cleaned_1d/tree_bi',
 'use_gpu': 'yes',
 'xent_regularize': 0.1}
2022-07-15 15:47:04,290 [/rmespch_train/sourya/kaldi/egs/tamil_telugu_proj/s5_r3/steps/nnet3/chain/train.py:341 - train - INFO ] Creating phone language-model
2022-07-15 15:47:07,889 [/rmespch_train/sourya/kaldi/egs/tamil_telugu_proj/s5_r3/steps/nnet3/chain/train.py:346 - train - INFO ] Creating denominator FST
copy-transition-model telugu_exp/chain_cleaned_1d/tree_bi/final.mdl telugu_exp/chain_cleaned_1d/tdnn1d_sp/0.trans_mdl 
LOG (copy-transition-model[5.5.1035~1-3dd90]:main():copy-transition-model.cc:62) Copied transition model.
2022-07-15 15:47:08,727 [/rmespch_train/sourya/kaldi/egs/tamil_telugu_proj/s5_r3/steps/nnet3/chain/train.py:353 - train - INFO ] Initializing a basic network for estimating preconditioning matrix
2022-07-15 15:47:08,791 [/rmespch_train/sourya/kaldi/egs/tamil_telugu_proj/s5_r3/steps/nnet3/chain/train.py:382 - train - INFO ] Generating egs
steps/nnet3/chain/get_egs.sh --frames-overlap-per-eg 0 --constrained false --online-cmvn true --cmd run.pl --cmvn-opts --config=conf/online_cmvn.conf --online-ivector-dir telugu_exp/nnet3_cleaned_1d/ivectors_train_cleaned_sp_hires --left-context 29 --right-context 29 --left-context-initial -1 --right-context-final -1 --left-tolerance 5 --right-tolerance 5 --frame-subsampling-factor 3 --alignment-subsampling-factor 3 --stage 0 --frames-per-iter 5000000 --frames-per-eg 150,110,100 --srand 0 telugu_data/train_cleaned_sp_hires telugu_exp/chain_cleaned_1d/tdnn1d_sp telugu_exp/chain_cleaned_1d/tri3_cleaned_train_cleaned_sp_lats telugu_exp/chain_cleaned_1d/tdnn1d_sp/egs
steps/nnet3/chain/get_egs.sh: File telugu_data/train_cleaned_sp_hires/utt2uniq exists, so ensuring the hold-out set includes all perturbed versions of the same source utterance.
steps/nnet3/chain/get_egs.sh: Holding out 300 utterances in validation set and 300 in training diagnostic set, out of total 133005.
steps/nnet3/chain/get_egs.sh: creating egs.  To ensure they are not deleted later you can do:  touch telugu_exp/chain_cleaned_1d/tdnn1d_sp/egs/.nodelete
steps/nnet3/chain/get_egs.sh: feature type is raw, with 'apply-cmvn-online'
tree-info telugu_exp/chain_cleaned_1d/tdnn1d_sp/tree 
feat-to-dim scp:telugu_exp/nnet3_cleaned_1d/ivectors_train_cleaned_sp_hires/ivector_online.scp - 
steps/nnet3/chain/get_egs.sh: working out number of frames of training data
steps/nnet3/chain/get_egs.sh: working out feature dim
steps/nnet3/chain/get_egs.sh: creating 9 archives, each with 46589 egs, with
steps/nnet3/chain/get_egs.sh:   150,110,100 labels per example, and (left,right) context = (29,29)
steps/nnet3/chain/get_egs.sh: Getting validation and training subset examples in background.
steps/nnet3/chain/get_egs.sh: Generating training examples on disk
steps/nnet3/chain/get_egs.sh: Getting subsets of validation examples for diagnostics and combination.
steps/nnet3/chain/get_egs.sh: recombining and shuffling order of archives on disk
steps/nnet3/chain/get_egs.sh: Removing temporary archives, alignments and lattices
steps/nnet3/chain/get_egs.sh: Finished preparing training examples
2022-07-15 15:50:02,075 [/rmespch_train/sourya/kaldi/egs/tamil_telugu_proj/s5_r3/steps/nnet3/chain/train.py:431 - train - INFO ] Copying the properties from telugu_exp/chain_cleaned_1d/tdnn1d_sp/egs to telugu_exp/chain_cleaned_1d/tdnn1d_sp
2022-07-15 15:50:02,145 [/rmespch_train/sourya/kaldi/egs/tamil_telugu_proj/s5_r3/steps/nnet3/chain/train.py:445 - train - INFO ] Computing the preconditioning matrix for input features
2022-07-15 15:50:33,212 [/rmespch_train/sourya/kaldi/egs/tamil_telugu_proj/s5_r3/steps/nnet3/chain/train.py:454 - train - INFO ] Preparing the initial acoustic model.
2022-07-15 15:50:41,008 [/rmespch_train/sourya/kaldi/egs/tamil_telugu_proj/s5_r3/steps/nnet3/chain/train.py:487 - train - INFO ] Training will run for 3.0 epochs = 81 iterations
2022-07-15 15:50:41,008 [/rmespch_train/sourya/kaldi/egs/tamil_telugu_proj/s5_r3/steps/nnet3/chain/train.py:529 - train - INFO ] Iter: 0/80   Jobs: 1   Epoch: 0.00/3.0 (0.0% complete)   lr: 0.000250   
bash: line 1: 584511 Aborted                 (core dumped) ( nnet3-chain-train --use-gpu=yes --apply-deriv-weights=False --l2-regularize=0.0 --leaky-hmm-coefficient=0.1 --write-cache=telugu_exp/chain_cleaned_1d/tdnn1d_sp/cache.1 --xent-regularize=0.1 --print-interval=10 --momentum=0.0 --max-param-change=1.414213562373095 --backstitch-training-scale=0.0 --backstitch-training-interval=1 --l2-regularize-factor=1.0 --optimization.memory-compression-level=2 --srand=0 "nnet3-am-copy --raw=true --learning-rate=0.00025 --scale=1.0 telugu_exp/chain_cleaned_1d/tdnn1d_sp/0.mdl - |nnet3-copy --edits='set-dropout-proportion name=* proportion=0.0' - - |" telugu_exp/chain_cleaned_1d/tdnn1d_sp/den.fst "ark,bg:nnet3-chain-copy-egs                          --frame-shift=1                         ark:telugu_exp/chain_cleaned_1d/tdnn1d_sp/egs/cegs.1.ark ark:- |                         nnet3-chain-shuffle-egs --buffer-size=5000                         --srand=0 ark:- ark:- | nnet3-chain-merge-egs                         --minibatch-size=32 ark:- ark:- |" telugu_exp/chain_cleaned_1d/tdnn1d_sp/1.1.raw ) 2>> telugu_exp/chain_cleaned_1d/tdnn1d_sp/log/train.0.1.log >> telugu_exp/chain_cleaned_1d/tdnn1d_sp/log/train.0.1.log
run.pl: job failed, log is in telugu_exp/chain_cleaned_1d/tdnn1d_sp/log/train.0.1.log
2022-07-15 15:50:55,656 [/rmespch_train/sourya/kaldi/egs/tamil_telugu_proj/s5_r3/steps/libs/common.py:207 - background_command_waiter - ERROR ] Command exited with status 1: run.pl --gpu 1 telugu_exp/chain_cleaned_1d/tdnn1d_sp/log/train.0.1.log                     nnet3-chain-train --use-gpu=yes                      --apply-deriv-weights=False                     --l2-regularize=0.0 --leaky-hmm-coefficient=0.1                      --write-cache=telugu_exp/chain_cleaned_1d/tdnn1d_sp/cache.1  --xent-regularize=0.1                                          --print-interval=10 --momentum=0.0                     --max-param-change=1.414213562373095                     --backstitch-training-scale=0.0                     --backstitch-training-interval=1                     --l2-regularize-factor=1.0 --optimization.memory-compression-level=2                      --srand=0                     "nnet3-am-copy --raw=true --learning-rate=0.00025 --scale=1.0 telugu_exp/chain_cleaned_1d/tdnn1d_sp/0.mdl - |nnet3-copy --edits='set-dropout-proportion name=* proportion=0.0' - - |" telugu_exp/chain_cleaned_1d/tdnn1d_sp/den.fst                     "ark,bg:nnet3-chain-copy-egs                          --frame-shift=1                         ark:telugu_exp/chain_cleaned_1d/tdnn1d_sp/egs/cegs.1.ark ark:- |                         nnet3-chain-shuffle-egs --buffer-size=5000                         --srand=0 ark:- ark:- | nnet3-chain-merge-egs                         --minibatch-size=32 ark:- ark:- |"                     telugu_exp/chain_cleaned_1d/tdnn1d_sp/1.1.raw
steps/nnet3/chain/train.py --stage -10 --cmd run.pl --feat.online-ivector-dir telugu_exp/nnet3_cleaned_1d/ivectors_train_cleaned_sp_hires --feat.cmvn-opts=--config=conf/online_cmvn.conf --chain.xent-regularize 0.1 --chain.leaky-hmm-coefficient 0.1 --chain.l2-regularize 0.0 --chain.apply-deriv-weights false --chain.lm-opts=--num-extra-lm-states=2000 --trainer.dropout-schedule 0,0@0.20,0.5@0.50,0 --trainer.add-option=--optimization.memory-compression-level=2 --egs.dir  --egs.opts --frames-overlap-per-eg 0 --constrained false --online-cmvn true --egs.chunk-width 150,110,100 --trainer.num-chunk-per-minibatch 64 --trainer.frames-per-iter 5000000 --trainer.num-epochs 3 --trainer.optimization.num-jobs-initial 1 --trainer.optimization.num-jobs-final 1 --trainer.optimization.initial-effective-lrate 0.00025 --trainer.optimization.final-effective-lrate 0.000025 --trainer.max-param-change 2.0 --cleanup.remove-egs true --feat-dir telugu_data/train_cleaned_sp_hires --tree-dir telugu_exp/chain_cleaned_1d/tree_bi --lat-dir telugu_exp/chain_cleaned_1d/tri3_cleaned_train_cleaned_sp_lats --dir telugu_exp/chain_cleaned_1d/tdnn1d_sp
['steps/nnet3/chain/train.py', '--stage', '-10', '--cmd', 'run.pl', '--feat.online-ivector-dir', 'telugu_exp/nnet3_cleaned_1d/ivectors_train_cleaned_sp_hires', '--feat.cmvn-opts=--config=conf/online_cmvn.conf', '--chain.xent-regularize', '0.1', '--chain.leaky-hmm-coefficient', '0.1', '--chain.l2-regularize', '0.0', '--chain.apply-deriv-weights', 'false', '--chain.lm-opts=--num-extra-lm-states=2000', '--trainer.dropout-schedule', '0,0@0.20,0.5@0.50,0', '--trainer.add-option=--optimization.memory-compression-level=2', '--egs.dir', '', '--egs.opts', '--frames-overlap-per-eg 0 --constrained false --online-cmvn true', '--egs.chunk-width', '150,110,100', '--trainer.num-chunk-per-minibatch', '64', '--trainer.frames-per-iter', '5000000', '--trainer.num-epochs', '3', '--trainer.optimization.num-jobs-initial', '1', '--trainer.optimization.num-jobs-final', '1', '--trainer.optimization.initial-effective-lrate', '0.00025', '--trainer.optimization.final-effective-lrate', '0.000025', '--trainer.max-param-change', '2.0', '--cleanup.remove-egs', 'true', '--feat-dir', 'telugu_data/train_cleaned_sp_hires', '--tree-dir', 'telugu_exp/chain_cleaned_1d/tree_bi', '--lat-dir', 'telugu_exp/chain_cleaned_1d/tri3_cleaned_train_cleaned_sp_lats', '--dir', 'telugu_exp/chain_cleaned_1d/tdnn1d_sp']
